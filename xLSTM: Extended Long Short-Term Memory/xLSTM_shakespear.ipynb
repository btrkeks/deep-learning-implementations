{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9912ed",
   "metadata": {
    "id": "1e9912ed"
   },
   "source": [
    "## Description\n",
    "\n",
    "This notebook implements a custom Extended Long Short-Term Memory (xLSTM) model to predict the next tokens given an input sequence as described in the paper [xLSTM: Extended Long Short-Term Memory](https://arxiv.org/abs/2405.04517).\n",
    "\n",
    "We will work with the “Tiny Shakespeare” dataset, a character-level corpus of Shakespeare’s plays and sonnets, commonly used for next-character prediction. The dataset is available at [Github](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584ba859",
   "metadata": {
    "id": "584ba859"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ce1eb1c",
   "metadata": {
    "id": "3ce1eb1c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Drive Setup for Checkpointing in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for checkpoint saving\n",
    "from google.colab import drive\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_dir = '/content/drive/MyDrive/xlstm_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Google Drive mounted successfully!\")\n",
    "print(f\"Checkpoint directory: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc3e99f",
   "metadata": {
    "id": "fcc3e99f"
   },
   "source": [
    "### **Preparing the Tokenizer and Dataloader** (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21572718",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21572718",
    "outputId": "e005b570-9278-4d87-8ff0-5cd0cd82ae89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n",
      "Dataset size: 1115330\n",
      "Training samples: 1003797\n",
      "Validation samples: 111533\n"
     ]
    }
   ],
   "source": [
    "# Character-level tokenizer\n",
    "class CharTokenizer:\n",
    "    def __init__(self, text):\n",
    "        self.chars = sorted(set(text))\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        self.vocab_size = len(self.chars)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.char_to_idx[ch] for ch in text]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return ''.join([self.idx_to_char[i] for i in indices])\n",
    "\n",
    "# Dataset class\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, seq_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "        self.data = tokenizer.encode(text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx+self.seq_length], dtype=torch.long)\n",
    "        y = torch.tensor(self.data[idx+1:idx+self.seq_length+1], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# Load and prepare data\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "tokenizer = CharTokenizer(text)\n",
    "seq_length = 64\n",
    "dataset = ShakespeareDataset(text, tokenizer, seq_length)\n",
    "\n",
    "# Split into train and validation\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8466d1",
   "metadata": {
    "id": "5f8466d1"
   },
   "source": [
    "### **Preparing the Model** (2.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56439879",
   "metadata": {
    "id": "56439879"
   },
   "source": [
    "#### components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89838416",
   "metadata": {
    "id": "89838416"
   },
   "outputs": [],
   "source": [
    "class BlockDiagonalProj(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads):\n",
    "        super(BlockDiagonalProj, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.out_head_size = input_dim // num_heads\n",
    "        self.weight = nn.Parameter(torch.empty(num_heads, self.out_head_size, input_dim // num_heads))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "        x = x.view(*shape[:-1], self.num_heads, -1)\n",
    "        x = torch.einsum(\"...hd,hod->...ho\", x, self.weight)\n",
    "        x = x.reshape(*shape[:-1], -1)\n",
    "        return x\n",
    "\n",
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, feature_dim, kernel_size, bias=True):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "        self.pad = (kernel_size -1)\n",
    "        self.conv = nn.Conv1d(in_channels=feature_dim, out_channels=feature_dim, kernel_size=kernel_size, padding=self.pad, groups=feature_dim, bias=bias)\n",
    "    def forward(self, x):\n",
    "        y = x.transpose(2, 1)\n",
    "        y = self.conv(y)\n",
    "        return y[:, :, : -self.pad].transpose(2, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b2eddd",
   "metadata": {
    "id": "50b2eddd"
   },
   "source": [
    "#### mLSTM block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a4ef7",
   "metadata": {
    "id": "b08a4ef7"
   },
   "outputs": [],
   "source": [
    "### COMPLETE THIS CLASS ####\n",
    "class mLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads):\n",
    "        super(mLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = input_dim // num_heads\n",
    "        \n",
    "        self.i_proj = BlockDiagonalProj(input_dim, num_heads)\n",
    "        self.f_proj = BlockDiagonalProj(input_dim, num_heads)\n",
    "\n",
    "    def forward(self, q, k, v, x_conv):  # Need conv output for gates\n",
    "        B, S, _ = q.shape\n",
    "\n",
    "        # Reshape to heads\n",
    "        q = q.view(B, S, self.num_heads, self.head_dim)\n",
    "        k = k.view(B, S, self.num_heads, self.head_dim)\n",
    "        v = v.view(B, S, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Get gates from conv output\n",
    "        i = self.i_proj(x_conv).view(B, S, self.num_heads, self.head_dim)\n",
    "        f = self.f_proj(x_conv).view(B, S, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Apply exponential activation to gates (while preventing overflow)\n",
    "        i = torch.exp(torch.clamp(i, max=10))\n",
    "        f = torch.exp(torch.clamp(f, max=10))\n",
    "\n",
    "        # Initialize states\n",
    "        C = torch.zeros(B, self.num_heads, self.head_dim, self.head_dim, device=q.device)\n",
    "        n = torch.zeros(B, self.num_heads, self.head_dim, device=q.device)\n",
    "        \n",
    "        outputs = []\n",
    "\n",
    "        for t in range(S):\n",
    "            qt = q[:, t]  # [B, H, D]\n",
    "            kt = k[:, t]  # [B, H, D]\n",
    "            vt = v[:, t]  # [B, H, D]\n",
    "            it = i[:, t]  # [B, H, D]\n",
    "            ft = f[:, t]  # [B, H, D]\n",
    "            \n",
    "            # Update memory matrix: C = f ⊙ C + i ⊙ (v ⊗ k)\n",
    "            # Expand gates for matrix operations\n",
    "            ft_expanded = ft.unsqueeze(-1)  # [B, H, D, 1]\n",
    "            it_expanded = it.unsqueeze(-1)  # [B, H, D, 1]\n",
    "            \n",
    "            vt_expanded = vt.unsqueeze(-1)  # [B, H, D, 1]\n",
    "            kt_expanded = kt.unsqueeze(-2)  # [B, H, 1, D]\n",
    "            \n",
    "            C = ft_expanded * C + it_expanded * (vt_expanded @ kt_expanded)\n",
    "            \n",
    "            # Update normalizer: n = f ⊙ n + i ⊙ k\n",
    "            n = ft * n + it * kt\n",
    "            \n",
    "            # Query processing with stabilization\n",
    "            h_tilde = torch.einsum('bhij,bhj->bhi', C, qt)  # [B, H, D]\n",
    "            \n",
    "            # Compute normalizer term\n",
    "            n_term = torch.abs(torch.einsum('bhd,bhd->bh', n, qt))  # [B, H]\n",
    "            n_term = torch.maximum(n_term, torch.ones_like(n_term))  # max(|q^T n|, 1)\n",
    "            \n",
    "            # Normalize output\n",
    "            h_t = h_tilde / n_term.unsqueeze(-1)  # [B, H, D]\n",
    "            \n",
    "            outputs.append(h_t)\n",
    "\n",
    "        # Stack outputs\n",
    "        output = torch.stack(outputs, dim=1)  # [B, S, H, D]\n",
    "        output = output.permute(0, 2, 1, 3)  # [B, H, S, D] for consistency\n",
    "        \n",
    "        return output\n",
    "#############################\n",
    "\n",
    "class mLSTMLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, proj_blocksize, bias=False):\n",
    "        super(mLSTMLayer, self).__init__()\n",
    "        self.outer_embedding_dim = embedding_dim\n",
    "        self.inner_embedding_dim = 2 * embedding_dim\n",
    "        self.proj_blocksize = proj_blocksize\n",
    "        self.bias = bias\n",
    "\n",
    "        self.proj_up = nn.Linear(in_features=self.outer_embedding_dim,\n",
    "                                 out_features= 2 * self.inner_embedding_dim,\n",
    "                                 bias=bias)\n",
    "        self.num_proj_heads = self.inner_embedding_dim // proj_blocksize\n",
    "        self.q_proj = BlockDiagonalProj(input_dim=self.inner_embedding_dim, num_heads=self.num_proj_heads)\n",
    "        self.k_proj = BlockDiagonalProj(input_dim=self.inner_embedding_dim, num_heads=self.num_proj_heads)\n",
    "        self.v_proj = BlockDiagonalProj(input_dim=self.inner_embedding_dim, num_heads=self.num_proj_heads)\n",
    "\n",
    "        self.conv1d = CausalConv1d(feature_dim=self.inner_embedding_dim, kernel_size=4)\n",
    "        self.conv_swish = nn.SiLU()\n",
    "\n",
    "        ############################     EDIT      ##################################\n",
    "        self.mlstm_cell = mLSTMCell(self.inner_embedding_dim, self.num_proj_heads)\n",
    "        ##############################################################\n",
    "\n",
    "        self.ogate_swish = nn.SiLU()\n",
    "        self.learnable_skip_con = nn.Parameter(torch.ones(self.inner_embedding_dim, requires_grad=True))\n",
    "        self.proj_down = nn.Linear(in_features=self.inner_embedding_dim,\n",
    "                                 out_features=self.outer_embedding_dim,\n",
    "                                 bias=bias)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S, _ = x.shape\n",
    "        x_ = F.layer_norm(x, normalized_shape=(self.outer_embedding_dim,))\n",
    "        x_inner = self.proj_up(x_)\n",
    "        x_mlstm, z = torch.split(x_inner, split_size_or_sections=self.inner_embedding_dim, dim=-1)\n",
    "        x_mlstm_conv = self.conv1d(x_mlstm)\n",
    "        x_mlstm_conv_act = self.conv_swish(x_mlstm_conv)\n",
    "\n",
    "        q = self.q_proj(x_mlstm_conv_act)\n",
    "        k = self.k_proj(x_mlstm_conv_act)\n",
    "        v = self.v_proj(x_mlstm)\n",
    "\n",
    "        ### EDIT ####\n",
    "        y_ = self.mlstm_cell(q, k, v, x_mlstm_conv_act)\n",
    "\n",
    "        B_, NH_, S_, DH_ = y_.shape\n",
    "        gn_in_1 = y_.transpose(1, 2)  # [B, S, NH, DH]\n",
    "        gn_in_2 = gn_in_1.reshape(B_ * S_, NH_ * DH_)  # [B*S, NH*DH]\n",
    "        gn_out = F.group_norm(gn_in_2, num_groups=NH_)\n",
    "        out = gn_out.view(B, S, -1)\n",
    "        #############\n",
    "\n",
    "        mlstm_cell_skip = out + (self.learnable_skip_con * x_mlstm_conv_act)\n",
    "\n",
    "        h_state = mlstm_cell_skip * self.ogate_swish(z)\n",
    "\n",
    "        y = self.proj_down(h_state) + x\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94243fd3",
   "metadata": {
    "id": "94243fd3"
   },
   "source": [
    "#### sLSTM block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e70c0d4",
   "metadata": {
    "id": "2e70c0d4"
   },
   "outputs": [],
   "source": [
    "### COMPLETE THIS CLASS ####\n",
    "class sLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads):\n",
    "        super(sLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = input_dim // num_heads\n",
    "\n",
    "    def forward(self, i, f, z, o):\n",
    "        B, S, D = i.shape\n",
    "\n",
    "        # Apply activations\n",
    "        i = torch.exp(torch.clamp(i, max=10))  # Prevent overflow\n",
    "        f = torch.exp(torch.clamp(f, max=10))  # Prevent overflow\n",
    "        z = torch.tanh(z)\n",
    "        o = torch.sigmoid(o)\n",
    "\n",
    "        # Reshape to heads\n",
    "        i = i.view(B, S, self.num_heads, self.head_dim)\n",
    "        f = f.view(B, S, self.num_heads, self.head_dim)\n",
    "        z = z.view(B, S, self.num_heads, self.head_dim)\n",
    "        o = o.view(B, S, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Initialize states\n",
    "        c = torch.zeros(B, self.num_heads, self.head_dim, device=i.device)\n",
    "        n = torch.zeros(B, self.num_heads, self.head_dim, device=i.device)\n",
    "        m = torch.zeros(B, self.num_heads, self.head_dim, device=i.device)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        # Process sequence\n",
    "        for t in range(S):\n",
    "            # Update states\n",
    "            c = f[:, t] * c + i[:, t] * z[:, t]\n",
    "            n = f[:, t] * n + i[:, t]\n",
    "            m = torch.maximum(f[:, t] * m + i[:, t], torch.abs(c))\n",
    "\n",
    "            # Compute output\n",
    "            h = o[:, t] * (c / (m + 1e-6))\n",
    "            outputs.append(h)\n",
    "\n",
    "        # Stack outputs and reshape properly\n",
    "        output = torch.stack(outputs, dim=1)  # [B, S, H, D]\n",
    "        output = output.view(B, self.num_heads, S, self.head_dim)  # [B, H, S, D]\n",
    "\n",
    "        return output\n",
    "#############################\n",
    "\n",
    "class sLSTMLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, proj_blocksize, conv_block=True, bias=False):\n",
    "        super(sLSTMLayer, self).__init__()\n",
    "        self.inner_embedding_dim = embedding_dim\n",
    "        self.proj_blocksize = proj_blocksize\n",
    "        self.conv_block = conv_block\n",
    "        self.num_heads = 4\n",
    "\n",
    "        if conv_block:\n",
    "            self.conv1d = CausalConv1d(feature_dim=self.inner_embedding_dim, kernel_size=4)\n",
    "            self.conv_swish = nn.SiLU()\n",
    "\n",
    "        self.i_proj = BlockDiagonalProj(input_dim=self.inner_embedding_dim, num_heads=self.num_heads)\n",
    "        self.f_proj = BlockDiagonalProj(input_dim=self.inner_embedding_dim, num_heads=self.num_heads)\n",
    "        self.z_proj = BlockDiagonalProj(input_dim=self.inner_embedding_dim, num_heads=self.num_heads)\n",
    "        self.o_proj = BlockDiagonalProj(input_dim=self.inner_embedding_dim, num_heads=self.num_heads)\n",
    "\n",
    "        self.slstm_cell = sLSTMCell(self.inner_embedding_dim, self.num_heads)\n",
    "\n",
    "        self.up_proj1 = nn.Linear(in_features=self.inner_embedding_dim, out_features= int((4/3)*self.inner_embedding_dim), bias=bias)\n",
    "        self.up_proj2 = nn.Linear(in_features=self.inner_embedding_dim, out_features= int((4/3)*self.inner_embedding_dim), bias=bias)\n",
    "        self.up_proj2_gelu = nn.GELU()\n",
    "\n",
    "        self.down_proj = nn.Linear(in_features=int((4/3)*self.inner_embedding_dim), out_features=self.inner_embedding_dim, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S, _ = x.shape\n",
    "\n",
    "        x_ = F.layer_norm(x, normalized_shape=(self.inner_embedding_dim,))\n",
    "\n",
    "        if self.conv_block:\n",
    "            x_conv = self.conv1d(x_)\n",
    "            x_conv_act = self.conv_swish(x_conv)\n",
    "        else:\n",
    "            x_conv_act = x_\n",
    "        i = self.i_proj(x_conv_act)\n",
    "        f = self.f_proj(x_conv_act)\n",
    "        z = self.z_proj(x_)\n",
    "        o = self.o_proj(x_)\n",
    "\n",
    "        y_ = self.slstm_cell(i, f, z, o)\n",
    "\n",
    "        B_, NH_, S_, DH_ = y_.shape\n",
    "        gn_in_1 = y_.transpose(1, 2)\n",
    "        gn_in_2 = gn_in_1.reshape(B_ * S_, NH_ * DH_)\n",
    "        gn_out = F.group_norm(gn_in_2, num_groups=NH_)\n",
    "        out = gn_out.view(B, S, -1)\n",
    "\n",
    "        skip_con = x + out\n",
    "        skip_con_layer_norm = F.layer_norm(skip_con, normalized_shape=(self.inner_embedding_dim,))\n",
    "\n",
    "        up_proj1 = self.up_proj1(skip_con_layer_norm)\n",
    "        up_proj2 = self.up_proj2(skip_con_layer_norm)\n",
    "        up_proj2_act = self.up_proj2_gelu(up_proj2)\n",
    "        down_proj = self.down_proj(up_proj2_act * up_proj1)\n",
    "        y = down_proj + skip_con\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc8173e-b9ae-42db-8b99-f1215acf149c",
   "metadata": {
    "id": "8fc8173e-b9ae-42db-8b99-f1215acf149c"
   },
   "source": [
    "xLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5e781f5-2132-4e06-9574-b4ba8b15c692",
   "metadata": {
    "id": "b5e781f5-2132-4e06-9574-b4ba8b15c692"
   },
   "outputs": [],
   "source": [
    "class xLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, num_layers=4, proj_blocksize=32):\n",
    "        super(xLSTM, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        nn.init.normal_(self.embedding.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        # xLSTM layers with 1:1 ratio (TODO: 7:1 is best according to paper)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                \"sLSTM\": sLSTMLayer(embedding_dim, proj_blocksize),\n",
    "                \"mLSTM\": mLSTMLayer(embedding_dim, proj_blocksize)\n",
    "            }) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "        nn.init.normal_(self.output_layer.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # [B, S, D]\n",
    "\n",
    "        # Pass through xLSTM layers\n",
    "        for layer_dict in self.layers:\n",
    "            # Apply sLSTM then mLSTM\n",
    "            x = layer_dict[\"sLSTM\"](x)\n",
    "            x = layer_dict[\"mLSTM\"](x)\n",
    "\n",
    "        # Output projection\n",
    "        output = self.output_layer(x)  # [B, S, vocab_size]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, train_losses, val_losses, train_perplexities, val_perplexities, checkpoint_dir):\n",
    "    \"\"\"Save model checkpoint to Google Drive\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'xlstm_checkpoint_epoch_{epoch+1}_{timestamp}.pt')\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_perplexities': train_perplexities,\n",
    "        'val_perplexities': val_perplexities,\n",
    "        'vocab_size': tokenizer.vocab_size,\n",
    "        'model_config': {\n",
    "            'embedding_dim': model.embedding_dim,\n",
    "            'vocab_size': model.vocab_size\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "    return checkpoint_path\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    \"\"\"Load model checkpoint from Google Drive\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    return (checkpoint['epoch'], \n",
    "            checkpoint['train_losses'], \n",
    "            checkpoint['val_losses'],\n",
    "            checkpoint['train_perplexities'], \n",
    "            checkpoint['val_perplexities'])\n",
    "\n",
    "def cleanup_old_checkpoints(checkpoint_dir, keep_last_n=3):\n",
    "    \"\"\"Keep only the N most recent checkpoints\"\"\"\n",
    "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'xlstm_checkpoint_*.pt'))\n",
    "    if len(checkpoint_files) > keep_last_n:\n",
    "        # Sort by modification time (newest first)\n",
    "        checkpoint_files.sort(key=os.path.getmtime, reverse=True)\n",
    "        # Remove older checkpoints\n",
    "        for old_checkpoint in checkpoint_files[keep_last_n:]:\n",
    "            os.remove(old_checkpoint)\n",
    "            print(f\"Removed old checkpoint: {old_checkpoint}\")\n",
    "\n",
    "def find_latest_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Find the most recent checkpoint\"\"\"\n",
    "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'xlstm_checkpoint_*.pt'))\n",
    "    if checkpoint_files:\n",
    "        return max(checkpoint_files, key=os.path.getmtime)\n",
    "    return None\n",
    "\n",
    "# Configuration\n",
    "SAVE_EVERY_N_BATCHES = 5000  # Save checkpoint every N batches\n",
    "RESUME_FROM_CHECKPOINT = False\n",
    "\n",
    "print(\"Checkpoint functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d0482",
   "metadata": {
    "id": "5a5d0482"
   },
   "source": [
    "### **Train the Model** (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f7af2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "160f7af2",
    "outputId": "f7a90e48-9aa8-4397-b8be-53e0a3a52b35"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Initialize model\n",
    "model = xLSTM(vocab_size=tokenizer.vocab_size, embedding_dim=128, num_layers=2, proj_blocksize=32)\n",
    "model = model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 3\n",
    "warmup_epochs = 1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create learning rate scheduler with warmup + cosine decay\n",
    "def create_warmup_cosine_scheduler(optimizer, warmup_epochs, total_epochs):\n",
    "    \"\"\"Create scheduler with linear warmup followed by cosine decay\"\"\"\n",
    "    from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "    \n",
    "    # Linear warmup from 0.1 to 1.0 of base LR\n",
    "    warmup_scheduler = LinearLR(\n",
    "        optimizer, \n",
    "        start_factor=0.1,\n",
    "        end_factor=1.0,\n",
    "        total_iters=warmup_epochs\n",
    "    )\n",
    "    \n",
    "    # Cosine annealing from peak to 10% of base LR\n",
    "    cosine_scheduler = CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=total_epochs - warmup_epochs,\n",
    "        eta_min=learning_rate * 0.1\n",
    "    )\n",
    "    \n",
    "    # Combine schedulers\n",
    "    scheduler = SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "        milestones=[warmup_epochs]\n",
    "    )\n",
    "    \n",
    "    return scheduler\n",
    "\n",
    "scheduler = create_warmup_cosine_scheduler(optimizer, warmup_epochs, num_epochs)\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_perplexities = []\n",
    "val_perplexities = []\n",
    "learning_rates = []  # Track learning rate changes\n",
    "\n",
    "# Resume from checkpoint if requested\n",
    "start_epoch = 0\n",
    "if RESUME_FROM_CHECKPOINT:\n",
    "    latest_checkpoint = find_latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(f\"Resuming from checkpoint: {latest_checkpoint}\")\n",
    "        start_epoch, train_losses, val_losses, train_perplexities, val_perplexities = load_checkpoint(\n",
    "            latest_checkpoint, model, optimizer)\n",
    "        start_epoch += 1  # Start from next epoch\n",
    "        print(f\"Resuming training from epoch {start_epoch + 1}\")\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting fresh training\")\n",
    "\n",
    "def calculate_perplexity(loss):\n",
    "    return math.exp(loss)\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "\n",
    "            # Reshape for loss calculation\n",
    "            outputs = outputs.view(-1, tokenizer.vocab_size)\n",
    "            y = y.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    num_train_batches = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        outputs = outputs.view(-1, tokenizer.vocab_size)\n",
    "        y = y.view(-1)\n",
    "\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        num_train_batches += 1\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            print(f'Epoch {epoch+1}/{start_epoch + num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}, LR: {current_lr:.6f}')\n",
    "        \n",
    "        # Save checkpoint every N batches\n",
    "        if (batch_idx + 1) % SAVE_EVERY_N_BATCHES == 0:\n",
    "            checkpoint_path = save_checkpoint(\n",
    "                model, optimizer, epoch, train_losses, val_losses, \n",
    "                train_perplexities, val_perplexities, checkpoint_dir\n",
    "            )\n",
    "            # Clean up old checkpoints\n",
    "            cleanup_old_checkpoints(checkpoint_dir, keep_last_n=3)\n",
    "\n",
    "    # Step the scheduler after each epoch\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    learning_rates.append(current_lr)\n",
    "\n",
    "    print(f'Finished epoch {epoch+1}')\n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = total_train_loss / num_train_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_perplexities.append(calculate_perplexity(avg_train_loss))\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    avg_val_loss = evaluate_model(model, val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_perplexities.append(calculate_perplexity(avg_val_loss))\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{start_epoch + num_epochs}:')\n",
    "    print(f'  Train Loss: {avg_train_loss:.4f}, Train Perplexity: {train_perplexities[-1]:.4f}')\n",
    "    print(f'  Val Loss: {avg_val_loss:.4f}, Val Perplexity: {val_perplexities[-1]:.4f}')\n",
    "    print(f'  Learning Rate: {current_lr:.6f}')\n",
    "    print('-' * 60)\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6077deee",
   "metadata": {
    "id": "6077deee"
   },
   "source": [
    "### **Showcasing plots and few input & output examples** (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c5351",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bf7c5351",
    "outputId": "462324ee-7b01-4066-f2e1-51f324d1eacd"
   },
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# Plot loss curves\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "ax1.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "ax1.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot perplexity curves\n",
    "ax2.plot(epochs, train_perplexities, 'b-', label='Training Perplexity')\n",
    "ax2.plot(epochs, val_perplexities, 'r-', label='Validation Perplexity')\n",
    "ax2.set_title('Training and Validation Perplexity')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Perplexity')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Plot learning rate schedule\n",
    "if learning_rates:\n",
    "    ax3.plot(epochs, learning_rates, 'g-', label='Learning Rate')\n",
    "    ax3.set_title('Learning Rate Schedule')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "    ax3.set_yscale('log')  # Log scale for better visualization\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Text generation function\n",
    "def generate_text(model, tokenizer, prompt, max_length=200, temperature=1.0):\n",
    "    model.eval()\n",
    "\n",
    "    # Encode the prompt\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    input_ids = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    generated = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Get model predictions\n",
    "            outputs = model(input_ids)\n",
    "\n",
    "            # Get the last token's logits\n",
    "            next_token_logits = outputs[0, -1, :] / temperature\n",
    "\n",
    "            # Sample from the distribution\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Add the new token to the sequence\n",
    "            generated.append(next_token.item())\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "            # Keep only the last seq_length tokens to avoid memory issues\n",
    "            if input_ids.size(1) > seq_length:\n",
    "                input_ids = input_ids[:, -seq_length:]\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(generated)\n",
    "    return prompt + generated_text\n",
    "\n",
    "# Generate some example texts\n",
    "print(\"=== Text Generation Examples ===\\n\")\n",
    "\n",
    "prompts = [\n",
    "    \"ROMEO:\",\n",
    "    \"To be or not to be,\",\n",
    "    \"HAMLET:\",\n",
    "    \"Fair is foul and\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    generated = generate_text(model, tokenizer, prompt, max_length=150, temperature=0.8)\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "\n",
    "print(\"=== Training Summary ===\")\n",
    "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Final Training Perplexity: {train_perplexities[-1]:.4f}\")\n",
    "print(f\"Final Validation Perplexity: {val_perplexities[-1]:.4f}\")\n",
    "if learning_rates:\n",
    "    print(f\"Final Learning Rate: {learning_rates[-1]:.6f}\")\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Vocabulary Size: {tokenizer.vocab_size}\")\n",
    "print(f\"Sequence Length: {seq_length}\")\n",
    "print(f\"Number of Training Samples: {len(train_dataset)}\")\n",
    "print(f\"Number of Validation Samples: {len(val_dataset)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
