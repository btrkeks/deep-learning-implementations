{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9912ed",
   "metadata": {
    "id": "1e9912ed"
   },
   "source": [
    "## Task Description\n",
    "\n",
    "In this exercise you will implement the `Speculative Decoding` algorithm for fast inference.\n",
    "The algorithm was proposed in the paper [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192) as Algorithm 1. Please get familiar with the paper and the algorithm before starting the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ea5031",
   "metadata": {
    "id": "a2ea5031"
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f34a09",
   "metadata": {
    "id": "94f34a09"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1fab8e",
   "metadata": {
    "id": "4a1fab8e"
   },
   "source": [
    "### 1. Implementation of the algorithm\n",
    "- Implement the `Speculative Decoding` algorithm as described in the paper.\n",
    "- Utilize TopK sampling inside the algorithm to sample from the draft model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71bf37e9",
   "metadata": {
    "id": "71bf37e9"
   },
   "outputs": [],
   "source": [
    "def sample(p, top_k=50, sample_rng=None):\n",
    "    assert len(p.shape) == 1\n",
    "\n",
    "    topk_probs, topk_indices = torch.topk(p, top_k, dim=-1)\n",
    "    ix = torch.multinomial(topk_probs, 1, generator=sample_rng)\n",
    "    xcol = torch.gather(topk_indices, -1, ix)\n",
    "    return xcol.view(1,1)\n",
    "\n",
    "def speculative_decoding_step(Mp, Mq, prefix_ids, gamma, tokenizer, device, top_k=50, sample_rng=None):\n",
    "    assert prefix_ids.shape[0] == 1, \"We assume batch size 1\"\n",
    "    seq_len = prefix_ids.shape[1]\n",
    "\n",
    "    # --- Step 1: Draft gamma tokens using Mq ---\n",
    "    draft_tokens = []\n",
    "    qs = []\n",
    "    current_ids = prefix_ids\n",
    "\n",
    "    for _ in range(gamma):\n",
    "        with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "            logits_q = Mq(current_ids).logits[0, -1, :]\n",
    "        probs_q = F.softmax(logits_q, dim=-1)\n",
    "        next_token = sample(probs_q, top_k, sample_rng)\n",
    "\n",
    "        draft_tokens.append(next_token)\n",
    "        qs.append(probs_q)\n",
    "\n",
    "        current_ids = torch.cat((current_ids, next_token), dim=-1)\n",
    "\n",
    "    # --- Step 2: Verify draft tokens with Mp in parallel ---\n",
    "    with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "        logits_p = Mp(current_ids).logits[0, seq_len-1:-1, :]\n",
    "    probs_p = F.softmax(logits_p, dim=-1)\n",
    "\n",
    "    # --- Step 3: Rejection Sampling ---\n",
    "    accepted_tokens = []\n",
    "    first_rejection_index = gamma # Assume all accepted initially\n",
    "\n",
    "    for i in range(gamma):\n",
    "        # Get the i-th draft token and the probabilities used/predicted for it\n",
    "        x = draft_tokens[i]\n",
    "        prob_q_x = qs[i][x]\n",
    "        prob_p_x = probs_p[i, x]\n",
    "\n",
    "        # Acceptance probability min(1, p/q)\n",
    "        accept_prob = torch.min(torch.ones_like(prob_p_x), prob_p_x / prob_q_x)\n",
    "\n",
    "        r = torch.rand(1, device=device, generator=sample_rng).item()\n",
    "\n",
    "        if r < accept_prob.item():\n",
    "            # Accept the token\n",
    "            accepted_tokens.append(draft_tokens[i])\n",
    "        else:\n",
    "            # Reject the token and break\n",
    "            first_rejection_index = i\n",
    "            break\n",
    "\n",
    "    # --- Step 4: Correction Sampling ---\n",
    "    final_tokens = accepted_tokens\n",
    "\n",
    "    if first_rejection_index == gamma:\n",
    "        # All draft tokens were accepted\n",
    "        last_logits_p = logits_p[-1, :]\n",
    "        last_probs_p = F.softmax(last_logits_p, dim=-1)\n",
    "        next_token = sample(last_probs_p, top_k, sample_rng)\n",
    "        final_tokens.append(next_token)\n",
    "    else:\n",
    "        # A token at index `first_rejection_index` was rejected.\n",
    "        p_k = probs_p[first_rejection_index, :] # p dist at rejected position k\n",
    "        q_k = qs[first_rejection_index] # q dist at rejected position k\n",
    "\n",
    "        # Calculate the corrected distribution: max(0, p - q)\n",
    "        corrected_probs = torch.max(torch.zeros_like(p_k), p_k - q_k)\n",
    "\n",
    "        # Normalize the corrected distribution\n",
    "        norm_factor = corrected_probs.sum(dim=-1, keepdim=True)\n",
    "        replacement_probs = corrected_probs / norm_factor\n",
    "\n",
    "        # Sample the replacement token\n",
    "        replacement_token = sample(replacement_probs, top_k, sample_rng)\n",
    "        final_tokens.append(replacement_token)\n",
    "\n",
    "    # Concatenate accepted/corrected tokens to the original prefix\n",
    "    new_suffix = torch.cat(final_tokens, dim=-1)\n",
    "    updated_ids = torch.cat([prefix_ids, new_suffix], dim=-1)\n",
    "\n",
    "    return updated_ids\n",
    "\n",
    "def inference_with_speculative_decoding(Mp, Mq, prompt, tokenizer, device, target_length):\n",
    "    sample_rng = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    tokens = tokens.unsqueeze(0)\n",
    "    tokens = tokens.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        step = 0\n",
    "        while tokens.shape[1] < target_length:\n",
    "            print(f\"Len tokens: {tokens.shape[1]}, target: {target_length}\")\n",
    "            new_tokens = speculative_decoding_step(Mp, Mq, tokens, 7, tokenizer, device, sample_rng=sample_rng)\n",
    "\n",
    "            num_generated = new_tokens.shape[1] - tokens.shape[1]\n",
    "            tokens = new_tokens\n",
    "            print(f\"Step {step}: Seq Len: {tokens.shape[1]}, Generated: {num_generated}\")\n",
    "            step += 1\n",
    "\n",
    "    return_tokens = tokens[0, :].tolist()\n",
    "    return tokenizer.decode(return_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bfcb88d",
   "metadata": {
    "id": "8bfcb88d"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "draft_model_name =  \"gpt2\"\n",
    "target_model_name = \"gpt2-medium\" # if this model is too large, you can use a smaller one like \"gpt2-large\" or \"gpt2-medium\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(draft_model_name)\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name)\n",
    "target_model = AutoModelForCausalLM.from_pretrained(target_model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "draft_model = draft_model.to(device).eval()\n",
    "target_model = target_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "665efb46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "665efb46",
    "outputId": "5876a8c0-60af-4f72-8ddb-70e0990e3784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draft model parameters: 124439808\n",
      "Target model parameters: 354823168\n"
     ]
    }
   ],
   "source": [
    "print(f\"Draft model parameters: {sum(p.numel() for p in draft_model.parameters())}\")\n",
    "print(f\"Target model parameters: {sum(p.numel() for p in target_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8f91bf",
   "metadata": {
    "id": "2c8f91bf"
   },
   "source": [
    "### 2. Integration of the algorithm into the inference pipeline\n",
    "Build an inference pipeline that uses the `Speculative Decoding` algorithm. Come up with some prefix which you can feed into the model as original input and generate some text with at least 100 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e51c1e1-32fa-4620-ac71-14bf26f9e9c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3e51c1e1-32fa-4620-ac71-14bf26f9e9c9",
    "outputId": "5914c010-ea9f-4c1c-8f21-864091de6b72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len tokens: 3, target: 100\n",
      "Step 0: Seq Len: 11, Generated: 8\n",
      "Len tokens: 11, target: 100\n",
      "Step 1: Seq Len: 12, Generated: 1\n",
      "Len tokens: 12, target: 100\n",
      "Step 2: Seq Len: 14, Generated: 2\n",
      "Len tokens: 14, target: 100\n",
      "Step 3: Seq Len: 22, Generated: 8\n",
      "Len tokens: 22, target: 100\n",
      "Step 4: Seq Len: 23, Generated: 1\n",
      "Len tokens: 23, target: 100\n",
      "Step 5: Seq Len: 25, Generated: 2\n",
      "Len tokens: 25, target: 100\n",
      "Step 6: Seq Len: 26, Generated: 1\n",
      "Len tokens: 26, target: 100\n",
      "Step 7: Seq Len: 30, Generated: 4\n",
      "Len tokens: 30, target: 100\n",
      "Step 8: Seq Len: 38, Generated: 8\n",
      "Len tokens: 38, target: 100\n",
      "Step 9: Seq Len: 41, Generated: 3\n",
      "Len tokens: 41, target: 100\n",
      "Step 10: Seq Len: 45, Generated: 4\n",
      "Len tokens: 45, target: 100\n",
      "Step 11: Seq Len: 46, Generated: 1\n",
      "Len tokens: 46, target: 100\n",
      "Step 12: Seq Len: 53, Generated: 7\n",
      "Len tokens: 53, target: 100\n",
      "Step 13: Seq Len: 56, Generated: 3\n",
      "Len tokens: 56, target: 100\n",
      "Step 14: Seq Len: 60, Generated: 4\n",
      "Len tokens: 60, target: 100\n",
      "Step 15: Seq Len: 64, Generated: 4\n",
      "Len tokens: 64, target: 100\n",
      "Step 16: Seq Len: 65, Generated: 1\n",
      "Len tokens: 65, target: 100\n",
      "Step 17: Seq Len: 73, Generated: 8\n",
      "Len tokens: 73, target: 100\n",
      "Step 18: Seq Len: 76, Generated: 3\n",
      "Len tokens: 76, target: 100\n",
      "Step 19: Seq Len: 83, Generated: 7\n",
      "Len tokens: 83, target: 100\n",
      "Step 20: Seq Len: 86, Generated: 3\n",
      "Len tokens: 86, target: 100\n",
      "Step 21: Seq Len: 89, Generated: 3\n",
      "Len tokens: 89, target: 100\n",
      "Step 22: Seq Len: 90, Generated: 1\n",
      "Len tokens: 90, target: 100\n",
      "Step 23: Seq Len: 94, Generated: 4\n",
      "Len tokens: 94, target: 100\n",
      "Step 24: Seq Len: 95, Generated: 1\n",
      "Len tokens: 95, target: 100\n",
      "Step 25: Seq Len: 96, Generated: 1\n",
      "Len tokens: 96, target: 100\n",
      "Step 26: Seq Len: 98, Generated: 2\n",
      "Len tokens: 98, target: 100\n",
      "Step 27: Seq Len: 101, Generated: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'A duck is a duck in the water. If One wants to learn to swim, One must understand it the way a duck understands its size and the way a turtle understands its motion.. And in the case of the individual who is on the water, he must not be deceived, that he think that there is anything further from \"nature\" than the human individual of which we are speaking. If he thinks such a question too easily, there will be no explanation for the individual\\'s position and his actions'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"A duck is\"\n",
    "target_len = 100\n",
    "\n",
    "inference_with_speculative_decoding(target_model, draft_model, prompt, tokenizer=tokenizer, device=device, target_length=target_len)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
