{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9912ed",
   "metadata": {
    "id": "1e9912ed"
   },
   "source": [
    "## Task Description\n",
    "\n",
    "In this exercise, you will implement Supervised Finetuning (SFT) for the pretrained GPT-2 model. You should use the `transformers` library to load the pretrained model and tokenizer. You will finetune the model on the `Alpaca` dataset, which is a collection of instruction-following examples. The dataset can be found [here](https://huggingface.co/datasets/tatsu-lab/alpaca).\n",
    "Your implementation should contain the four parts specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "jsp-M6-olDc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsp-M6-olDc3",
    "outputId": "2bb63328-6a7e-4b4f-9937-baea2131f8d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c44df2a",
   "metadata": {
    "id": "4c44df2a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "iS3zZagaI6jn",
   "metadata": {
    "id": "iS3zZagaI6jn"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-medium\"\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 2\n",
    "MAX_LENGTH = 256 # Max sequence length for tokenization\n",
    "LEARNING_RATE = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ydL1CM3dFP6S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydL1CM3dFP6S",
    "outputId": "dc980c2f-bb97-449a-a51a-9e088c70aeff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "LcyKQAKBFcM0",
   "metadata": {
    "id": "LcyKQAKBFcM0"
   },
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "class AlpacaDataset(Dataset):\n",
    "    def __init__(self, data_points, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.instructions = [item['instruction'] for item in data_points]\n",
    "        self.inputs = [item.get('input', \"\") for item in data_points]\n",
    "        self.outputs = [item['output'] for item in data_points]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instructions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instruction = self.instructions[idx]\n",
    "        input_text = self.inputs[idx]\n",
    "        output_text = self.outputs[idx]\n",
    "\n",
    "        if input_text:\n",
    "            prompt_source = PROMPT_DICT[\"prompt_input\"].format(instruction=instruction, input=input_text)\n",
    "        else:\n",
    "            prompt_source = PROMPT_DICT[\"prompt_no_input\"].format(instruction=instruction)\n",
    "\n",
    "        full_text = prompt_source + \" \" + output_text + self.tokenizer.eos_token\n",
    "\n",
    "        tokenized_full = self.tokenizer(\n",
    "            full_text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=False, # Collator will handle padding\n",
    "            return_tensors=None # Collator will convert to tensors\n",
    "        )\n",
    "\n",
    "        tokenized_prompt_source = self.tokenizer(\n",
    "            prompt_source,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized_full[\"input_ids\"]\n",
    "        attention_mask = tokenized_full[\"attention_mask\"]\n",
    "        labels = list(input_ids)\n",
    "\n",
    "        prompt_len = len(tokenized_prompt_source[\"input_ids\"])\n",
    "\n",
    "        for i in range(min(prompt_len, len(labels))):\n",
    "            labels[i] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0R9jX91xLrTE",
   "metadata": {
    "id": "0R9jX91xLrTE"
   },
   "outputs": [],
   "source": [
    "class DataCollatorForSFT:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        input_ids_list = [item['input_ids'] for item in batch]\n",
    "        attention_mask_list = [item['attention_mask'] for item in batch]\n",
    "        labels_list = [item['labels'] for item in batch]\n",
    "\n",
    "        # Determine the maximum sequence length in the current batch\n",
    "        max_len_in_batch = max(len(seq) for seq in input_ids_list)\n",
    "\n",
    "        padded_input_ids = []\n",
    "        padded_attention_mask = []\n",
    "        padded_labels = []\n",
    "\n",
    "        # Pad each sequence in the batch to max_len_in_batch\n",
    "        for i in range(len(batch)):\n",
    "            input_ids = input_ids_list[i]\n",
    "            attention_mask = attention_mask_list[i]\n",
    "            labels = labels_list[i]\n",
    "\n",
    "            padding_length = max_len_in_batch - len(input_ids)\n",
    "\n",
    "            # Right-pad input_ids with pad_token_id\n",
    "            padded_input_ids.append(input_ids + [self.tokenizer.pad_token_id] * padding_length)\n",
    "            # Right-pad attention_mask with 0\n",
    "            padded_attention_mask.append(attention_mask + [0] * padding_length)\n",
    "            # Right-pad labels with -100\n",
    "            padded_labels.append(labels + [-100] * padding_length)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(padded_input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(padded_attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(padded_labels, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "D3tww9SwNtAW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3tww9SwNtAW",
    "outputId": "9f307fbf-4ae6-4ecd-8e9a-ecd83df31639"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "wMw5UkJCO_Ba",
   "metadata": {
    "id": "wMw5UkJCO_Ba"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "Gzm_SKtuN4Om",
   "metadata": {
    "id": "Gzm_SKtuN4Om"
   },
   "outputs": [],
   "source": [
    "subset_alpaca_data = dataset.select(range(2000))\n",
    "\n",
    "train_dataset = AlpacaDataset(subset_alpaca_data, tokenizer, MAX_LENGTH)\n",
    "data_collator = DataCollatorForSFT(tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bORQWKpJAYY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "cf12e88a1f14480e92c9c45a189b6205",
      "e792f0e542524d16b56bc85b88c344cd",
      "7b32597cd2b84a33a750f2aed28cc3fd",
      "e93c3b184abf41b0a5f099641f84a549",
      "d57d96dd5701465aab18e26dce32e9ff",
      "dee9fb5e27f64cb6a1563ae51d0d8c9a",
      "23ba09d2d5234bdfaef84f0a0fa4c1e1",
      "acd2650ab98442a084a3e29a3ac3d590",
      "18c9a3d63dc24534b889672665080c16",
      "b5815223d8f34c01a7aa41549e45e260",
      "c85a143c8550425b9e2c3a355cf58ec6"
     ]
    },
    "id": "5bORQWKpJAYY",
    "outputId": "35430a42-1694-4e65-de9f-f11538ee5335"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf12e88a1f14480e92c9c45a189b6205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "        loss = loss / GRAD_ACCUM_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cMc4fmthbbCB",
   "metadata": {
    "id": "cMc4fmthbbCB"
   },
   "outputs": [],
   "source": [
    "def generate_answer(instruction):\n",
    "    prompt_text = PROMPT_DICT[\"prompt_no_input\"].format(instruction=instruction)\n",
    "\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", max_length=MAX_LENGTH, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=70,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "    # Decode only the newly generated part of the sequence\n",
    "    generated_tokens = output_sequences[0][prompt_len:]\n",
    "    generated_answer = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return generated_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8lqaQ4S_KIzk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8lqaQ4S_KIzk",
    "outputId": "59f817c8-548a-459e-da42-d38d981a3b7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pair 1:\n",
      "Question: Write a short story about a robot learning to paint.\n",
      "Generated Answer:  The robot was learning to paint by using its mind to learn and master the patterns it had seen. It began to paint by the patterns it could see, and the patterns it could see were becoming more and more accurate.\n",
      "\n",
      "Pair 2:\n",
      "Question: Explain the difference between 'less' and 'fewer' with examples.\n",
      "Generated Answer:  Less is 'less' than 'fewer'.\n",
      "\n",
      "Pair 3:\n",
      "Question: Provide three tips for improving concentration while studying.\n",
      "Generated Answer:  1. Practice focusing on one task at a time.\n",
      "2. Practice practicing concentration on a single task in one task.\n",
      "3. Practice a few simple exercises such as breathing, stretching, and concentrating on one word or phrase.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "generated_pairs = []\n",
    "\n",
    "\n",
    "generation_prompts_data = [\n",
    "    {\"instruction\": \"Write a short story about a robot learning to paint.\", \"input\": \"\"},\n",
    "    {\"instruction\": \"Explain the difference between 'less' and 'fewer' with examples.\", \"input\": \"\"},\n",
    "    {\"instruction\": \"Provide three tips for improving concentration while studying.\", \"input\": \"\"}\n",
    "]\n",
    "\n",
    "for i, item in enumerate(generation_prompts_data):\n",
    "    instruction = item['instruction']\n",
    "    generated_answer = generate_answer(instruction)\n",
    "\n",
    "    print(f\"\\nPair {i+1}:\")\n",
    "    print(f\"Question: {instruction}\")\n",
    "    print(f\"Generated Answer: {generated_answer}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "18c9a3d63dc24534b889672665080c16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "23ba09d2d5234bdfaef84f0a0fa4c1e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b32597cd2b84a33a750f2aed28cc3fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_acd2650ab98442a084a3e29a3ac3d590",
      "max": 2000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_18c9a3d63dc24534b889672665080c16",
      "value": 2000
     }
    },
    "acd2650ab98442a084a3e29a3ac3d590": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5815223d8f34c01a7aa41549e45e260": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c85a143c8550425b9e2c3a355cf58ec6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf12e88a1f14480e92c9c45a189b6205": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e792f0e542524d16b56bc85b88c344cd",
       "IPY_MODEL_7b32597cd2b84a33a750f2aed28cc3fd",
       "IPY_MODEL_e93c3b184abf41b0a5f099641f84a549"
      ],
      "layout": "IPY_MODEL_d57d96dd5701465aab18e26dce32e9ff"
     }
    },
    "d57d96dd5701465aab18e26dce32e9ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dee9fb5e27f64cb6a1563ae51d0d8c9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e792f0e542524d16b56bc85b88c344cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dee9fb5e27f64cb6a1563ae51d0d8c9a",
      "placeholder": "​",
      "style": "IPY_MODEL_23ba09d2d5234bdfaef84f0a0fa4c1e1",
      "value": "Epoch 1: 100%"
     }
    },
    "e93c3b184abf41b0a5f099641f84a549": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5815223d8f34c01a7aa41549e45e260",
      "placeholder": "​",
      "style": "IPY_MODEL_c85a143c8550425b9e2c3a355cf58ec6",
      "value": " 2000/2000 [07:41&lt;00:00,  4.50it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
